{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"detl (Declarative ETL) <p>A rigorous, declarative CLI tool and Python API explicitly designed to perform rapid data validation, transformation, and cleansing using strictly typed YAML Data Contracts.</p>    [![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&amp;logo=python)](https://python.org)   [![Polars](https://img.shields.io/badge/Powered_By-Polars-orange?style=for-the-badge)](https://pola.rs)   [![Pydantic](https://img.shields.io/badge/Validation-Pydantic-e92063?style=for-the-badge)](https://docs.pydantic.dev)   [![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)  <p>Powered natively by Polars (for blazing-fast, C++ backed <code>LazyFrame</code> memory mapping) and Pydantic (for strict, Pythonic data validations), <code>detl</code> allows Data Engineers to stop writing 1000-line monolithic scripts that ingest data and instead rely on decoupled configuration streams and <code>Source</code>/<code>Sink</code> protocols.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Data Contracts as Code: Configure pipelines gracefully over YAML. Validate nulls, schemas, strings, regex, ranges, formats, and duplicates safely.</li> <li>Pluggable Connector Ecosystem: Read and write straight from <code>S3 (MinIO/AWS)</code>, <code>Postgres</code>, <code>MySQL</code>, <code>SQLite</code>, <code>Parquet</code>, <code>Excel</code>, and <code>CSV</code> files natively.</li> <li>Zero-Copy Memory: Direct SQL mappings into Polars engines utilizing state-of-the-art native <code>adbc</code> and <code>connectorx</code> rust accelerators.</li> <li>Modular Python API: Fully accessible in Airflow, Prefect, or any standard application logic.</li> <li>Strict Domain Exceptions: Predictable exception tracebacks mapping to exactly what constraint violated the dataset (<code>NullViolationError</code>, <code>DuplicateRowError</code>).</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Ensure your environment complies with Python 3.10+ and utilizes modern packaging:</p> <pre><code># Core installation\nuv pip install -e .\n\n# Extend with zero-copy database &amp; cloud adapters (Recommended)\nuv pip install connectorx adbc-driver-postgresql adbc-driver-sqlite pymysql sqlalchemy pandas boto3\n</code></pre>"},{"location":"#global-cli-usage","title":"\ufe0f Global CLI Usage","text":"<p>You don't even need to write Python files to process Gigabytes of Data securely. Just deploy a YAML contract and pipe your DB bounds:</p>"},{"location":"#database-ingestion-to-parquet-cloud-archiving","title":"Database Ingestion to Parquet Cloud Archiving","text":"<pre><code>detl --config contract.yml \\\n     --source-type postgres \\\n     --source-uri \"postgresql://admin:password@localhost:5432/analytics\" \\\n     --source-query \"SELECT * FROM raw_telemetry WHERE active = true\" \\\n     --source-batch-size 100000 \\\n     --sink-type s3 \\\n     --sink-uri \"s3://telemetry-lake/clean_output.parquet\"\n</code></pre>"},{"location":"#python-library-orchestration","title":"Python Library Orchestration","text":"<p>Integrating <code>detl</code> directly into your existing backends is remarkably easy using the decoupled API mappings.</p> <pre><code>from detl import Processor, Config\nfrom detl.connectors import PostgresSource, S3Sink\n\n# 1. Digest the Pipeline Contract\nconfig = Config(\"contract.yml\")\n\n# 2. Wire the Input and Output boundaries (Files, DBs, and S3 are all supported universally)\nsource = PostgresSource(\n    connection_uri=\"postgresql://user:pass@localhost:5432/db\", \n    query=\"SELECT * FROM raw_events\"\n)\nsink = S3Sink(\n    s3_uri=\"s3://data-lake/clean_events.parquet\",\n    format=\"parquet\"\n)\n\n# 3. Spin up the Processor and execute the mapping!\nproc = Processor(config)\nproc.execute(source, sink)\n</code></pre>"},{"location":"#comprehensive-documentation","title":"Comprehensive Documentation","text":"<p>To master Data Contract structures and the Connector API, inspect our documentation ecosystem:</p> <p>Check <code>examples/03_kitchen_sink.yml</code> for a highly documented example of everything all at once.</p>"},{"location":"07_connector_api_reference/","title":"7. Connector API &amp; CLI Reference","text":"<p><code>detl</code> uses a fully decoupled architecture routing data through the <code>detl.connectors</code> abstraction boundaries. What makes <code>detl</code> unique is Matrix Routing: you can pipe data from any Source to any Sink directly without ever saving intermediate files locally!</p>"},{"location":"07_connector_api_reference/#cli-crash-course-matrix-routing","title":"CLI Crash Course (Matrix Routing)","text":"<p>The true power of <code>detl</code> is executed via the CLI. All operations require a path to the Contract (<code>-f config.yaml</code>).</p>"},{"location":"07_connector_api_reference/#1-local-files-the-defaults","title":"1. Local Files (The Defaults)","text":"<p>Use <code>-i</code> and <code>-o</code> as quick shortcuts for File System connectors (CSV, Parquet, Excel).</p> <pre><code># Convert a CSV to Parquet while applying validation schema\nuv run detl -f conf.yaml -i raw_data.csv -o clean_data.parquet\n</code></pre>"},{"location":"07_connector_api_reference/#2-database-extraction","title":"2. Database Extraction","text":"<p>Use <code>--source-type</code> and <code>--source-uri</code> to execute SQL queries natively directly into the pipeline!</p> <pre><code># Extract messy records from Postgres, clean them, and save to CSV\nuv run detl -f conf.yaml \\\n  --source-type postgres \\\n  --source-uri \"postgresql://user:password@localhost:5432/detldb\" \\\n  --source-query \"SELECT * FROM users\" \\\n  -o clean_db.csv\n</code></pre>"},{"location":"07_connector_api_reference/#3-database-to-database-or-db-to-s3","title":"3. Database to Database (Or DB to S3)","text":"<p>You don't need <code>-o</code>! Just specify <code>--sink-type</code> to push data across domains.</p> <pre><code># Read from Postgres -&gt; Validate -&gt; Write cleanly into MySQL\nuv run detl -f conf.yaml \\\n  --source-type postgres \\\n  --source-uri \"postgresql://user:pass@localhost:5432/db1\" \\\n  --source-query \"SELECT * FROM public.orders\" \\\n  --sink-type mysql \\\n  --sink-uri \"mysql+pymysql://user:pass@localhost:3306/db2\" \\\n  --sink-table \"clean_orders\"\n</code></pre>"},{"location":"07_connector_api_reference/#cloud-connectors-s3-minio","title":"Cloud Connectors (S3 / MinIO)","text":"<p>Transfers data in-memory directly utilizing strictly <code>boto3</code>. No local disk space is ever required during pipeline routing.</p>"},{"location":"07_connector_api_reference/#cli-arguments","title":"CLI Arguments","text":"<ul> <li><code>--source-type s3</code> / <code>--sink-type s3</code></li> <li><code>--source-uri \"s3://...\"</code> / <code>--sink-uri \"s3://...\"</code></li> <li><code>--s3-endpoint-url \"http://localhost:9000\"</code> (Optional modifier for custom domains like MinIO)</li> </ul> <pre><code>uv run detl -f conf.yaml -i local.csv --sink-type s3 --sink-uri \"s3://my-bucket/datalake.parquet\" --s3-endpoint-url \"http://localhost:9000\"\n</code></pre>"},{"location":"07_connector_api_reference/#python-api","title":"Python API","text":"<pre><code>from detl.connectors import S3Source, S3Sink\n\nsource = S3Source(\n    s3_uri=\"s3://my-bucket/data.parquet\",\n    format=\"parquet\", # 'csv' or 'parquet'\n    aws_access_key_id=\"admin\", # Or relies on ENV vars natively\n    aws_secret_access_key=\"password\",\n    endpoint_url=\"http://localhost:9000\" # For MinIO\n)\n\nsink = S3Sink(\n    s3_uri=\"s3://my-bucket/cleaned.parquet\",\n    format=\"parquet\"\n)\n</code></pre>"},{"location":"07_connector_api_reference/#database-connectors-postgres-mysql-sqlite","title":"Database Connectors (Postgres / MySQL / SQLite)","text":"<p>Native database protocols mapping zero-copy direct queries via <code>connectorx</code> reading protocols and <code>adbc</code> / <code>sqlalchemy</code> optimized writing blocks.</p>"},{"location":"07_connector_api_reference/#cli-arguments_1","title":"CLI Arguments","text":"<ul> <li><code>--source-type postgres</code> | <code>mysql</code> | <code>sqlite</code></li> <li><code>--source-uri \"protocol://credentials...\"</code></li> <li><code>--source-query \"SELECT * FROM...\"</code></li> <li><code>--source-batch-size 1000</code> (Optional)</li> <li><code>--sink-type postgres</code> | <code>mysql</code> | <code>sqlite</code></li> <li><code>--sink-uri \"protocol://credentials...\"</code></li> <li><code>--sink-table \"target_table_name\"</code></li> <li><code>--sink-if-exists replace</code> (Choices: <code>replace</code> (default), <code>append</code>, <code>fail</code>. Dictates idempotency)</li> <li><code>--sink-batch-size 5000</code> (Optional. Caps streaming memory consumption)</li> </ul>"},{"location":"07_connector_api_reference/#fault-tolerance-memory-idempotency","title":"Fault Tolerance, Memory &amp; Idempotency","text":"<p>Database extractors are strictly typed to act predictably when pipelining large data volumes: 1. Idempotency Defaults (<code>--sink-if-exists replace</code>): Running <code>detl</code> twice against the same data pipeline guarantees identical outcomes natively by safely tearing down the target <code>sink-table</code> and rewriting schemas to match the manifest. Override passing <code>append</code> to map new valid entries sequentially. 2. Memory Overflows (<code>--...-batch-size 50000</code>): Avoid OOM (Out Of Memory) node crashes when extracting millions of rows via <code>PostgresSource</code> by strictly declaring the maximum chunk limits. <code>detl</code> streams data in constrained iterations. 3. Hard Failures: If the <code>source</code> table is missing, the API crashes immediately with <code>[error]Source/Sink Connection Error:[/error]</code>, preventing \"phantom runs\". Null tables return error outputs matching strict DB paradigms.</p>"},{"location":"07_connector_api_reference/#python-api_1","title":"Python API","text":"<pre><code>from detl.connectors import PostgresSource, MySQLSink\n\nsource = PostgresSource(\n    connection_uri=\"postgresql://user:pass@host:5432/db\",\n    query=\"SELECT id, name FROM users\",\n    batch_size=50000 # Optional chunking size constraint\n)\n\nsink = MySQLSink(\n    connection_uri=\"mysql://user:pass@host:3306/db\",\n    table_name=\"cleaned_users\",\n    if_table_exists=\"replace\", # 'append' or 'fail'\n    batch_size=100000\n)\n</code></pre> <p>Dependency Requirements: *   Postgres Reads: <code>connectorx</code>, <code>adbc-driver-postgresql</code> *   MySQL Reads: <code>connectorx</code> *   MySQL Writes: <code>sqlalchemy</code>, <code>pymysql</code>, <code>pandas</code> (Pandas utilized purely as an abstraction fallback) *   SQLite: <code>adbc-driver-sqlite</code></p>"},{"location":"07_connector_api_reference/#file-system-connectors-csv-parquet-excel","title":"File System Connectors (CSV / Parquet / Excel)","text":"<p>Polars-native highly tuned binary abstractions pointing directly at physical disk targets. Automatically evaluates formats lazily avoiding out-of-memory evaluation errors.</p>"},{"location":"07_connector_api_reference/#python-api_2","title":"Python API","text":"<pre><code>from detl.connectors import CsvSource, ParquetSink\n\nsource = CsvSource(\n    path=\"./local_data/input.csv\",\n    separator=\",\" # Auto-defaults to comma\n)\n\nsink = ParquetSink(\n    path=\"./local_data/output_cleaned.parquet\",\n    streaming=True # Enforces streaming API evaluation dynamically against LazyFrames\n)\n</code></pre>"}]}