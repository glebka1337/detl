{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"detl (Declarative ETL) <p>A rigorous, declarative CLI tool and Python API explicitly designed to perform rapid data validation, transformation, and cleansing using strictly typed YAML Data Contracts.</p>    [![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&amp;logo=python)](https://python.org)   [![Polars](https://img.shields.io/badge/Powered_By-Polars-orange?style=for-the-badge)](https://pola.rs)   [![Pydantic](https://img.shields.io/badge/Validation-Pydantic-e92063?style=for-the-badge)](https://docs.pydantic.dev)   [![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)  <p>Powered natively by Polars (for blazing-fast, C++ backed <code>LazyFrame</code> memory mapping) and Pydantic (for strict, Pythonic data validations), <code>detl</code> allows Data Engineers to stop writing 1000-line monolithic scripts that ingest data and instead rely on decoupled configuration streams and <code>Source</code>/<code>Sink</code> protocols.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Data Contracts as Code: Configure pipelines gracefully over YAML. Validate nulls, schemas, strings, regex, ranges, formats, and duplicates safely.</li> <li>Pluggable Connector Ecosystem: Read and write straight from <code>S3 (MinIO/AWS)</code>, <code>Postgres</code>, <code>MySQL</code>, <code>SQLite</code>, <code>Parquet</code>, <code>Excel</code>, and <code>CSV</code> files natively.</li> <li>Zero-Copy Memory: Direct SQL mappings into Polars engines utilizing state-of-the-art native <code>adbc</code> and <code>connectorx</code> rust accelerators.</li> <li>Modular Python API: Fully accessible in Airflow, Prefect, or any standard application logic.</li> <li>Strict Domain Exceptions: Predictable exception tracebacks mapping to exactly what constraint violated the dataset (<code>NullViolationError</code>, <code>DuplicateRowError</code>).</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Ensure your environment complies with Python 3.10+ and utilizes modern packaging:</p> <pre><code># Core installation\nuv pip install -e .\n\n# Extend with zero-copy database &amp; cloud adapters (Recommended)\nuv pip install connectorx adbc-driver-postgresql adbc-driver-sqlite pymysql sqlalchemy pandas boto3\n</code></pre>"},{"location":"#global-cli-usage","title":"\ufe0f Global CLI Usage","text":"<p>You don't even need to write Python files to process Gigabytes of Data securely. Just deploy a YAML contract and pipe your DB bounds:</p>"},{"location":"#database-ingestion-to-parquet-cloud-archiving","title":"Database Ingestion to Parquet Cloud Archiving","text":"<pre><code>detl --config contract.yml \\\n     --source-type postgres \\\n     --source-uri \"postgresql://admin:password@localhost:5432/analytics\" \\\n     --source-query \"SELECT * FROM raw_telemetry WHERE active = true\" \\\n     --source-batch-size 100000 \\\n     --sink-type s3 \\\n     --sink-uri \"s3://telemetry-lake/clean_output.parquet\"\n</code></pre>"},{"location":"#python-library-orchestration","title":"Python Library Orchestration","text":"<p>Integrating <code>detl</code> directly into your existing backends is remarkably easy using the decoupled API mappings.</p> <pre><code>from detl import Processor, Config\nfrom detl.connectors import PostgresSource, S3Sink\n\n# 1. Digest the Pipeline Contract\nconfig = Config(\"contract.yml\")\n\n# 2. Wire the Input and Output boundaries (Files, DBs, and S3 are all supported universally)\nsource = PostgresSource(\n    connection_uri=\"postgresql://user:pass@localhost:5432/db\", \n    query=\"SELECT * FROM raw_events\"\n)\nsink = S3Sink(\n    s3_uri=\"s3://data-lake/clean_events.parquet\",\n    format=\"parquet\"\n)\n\n# 3. Spin up the Processor and execute the mapping!\nproc = Processor(config)\nproc.execute(source, sink)\n</code></pre>"},{"location":"#comprehensive-documentation","title":"Comprehensive Documentation","text":"<p>To master Data Contract structures and the Connector API, inspect our documentation ecosystem:</p> <p>Check <code>examples/03_kitchen_sink.yml</code> for a highly documented example of everything all at once.</p>"},{"location":"01_configuration/","title":"1. Global Engine Configuration (<code>conf</code>)","text":"<p>The <code>conf</code> block dictates early engine behaviors regarding overall dataset structure anomalies. It's the very first block in your YAML file.</p>"},{"location":"01_configuration/#undefined_columns","title":"<code>undefined_columns</code>","text":"<p>Determines what the engine does to columns found in the input DataFrame that are not present in the <code>columns</code> schema block. - <code>drop</code> (Recommended): Safely ignores extra columns. - <code>keep</code>: Allows unmapped columns to pass through untouched.</p> <p>DO (Strict Contract): <pre><code>conf:\n  undefined_columns: \"drop\" # Protects downstream tables from unexpected schema bloat!\n</code></pre></p> <p>DON'T (Loose Contract): <pre><code>conf:\n  undefined_columns: \"keep\" # Bad idea. If the source file accidentally contains 20 extra garbage columns, they will flow right into your database.\n</code></pre></p>"},{"location":"01_configuration/#on_duplicate_rows","title":"<code>on_duplicate_rows</code>","text":"<p>Determines how exact or subset-match duplicate rows are handled. Contains <code>tactic</code> and an optional <code>subset</code> list. - <code>tactic</code>:   - <code>keep</code>: Do nothing.   - <code>drop_extras</code> (Recommended): Drops subsequent duplicates, retaining the first occurrence.   - <code>fail</code>: Halts execution immediately if duplicates are identified. - <code>subset</code>: <code>[\"col_name1\", \"col_name2\"]</code> \u2014 If provided, duplication is determined strictly based on this combination. If omitted, duplication is evaluated across all columns.</p> <p>DO (Smart Deduplication): <pre><code>conf:\n  on_duplicate_rows:\n    tactic: \"drop_extras\"\n    subset: [\"user_id\", \"email\"] # Only drops rows if BOTH user_id and email are identical.\n</code></pre></p> <p>DON'T (Over-strict execution): <pre><code>conf:\n  on_duplicate_rows:\n    tactic: \"fail\" # DON'T DO THIS on raw data sources! A single accidental duplicate will crash the engine on a 50GB file. Let the engine silently clean it using 'drop_extras'.\n</code></pre></p>"},{"location":"01_configuration/#defaults","title":"<code>defaults</code>","text":"<p>Provides the ability to map global type-level fallback policies. This is extremely powerful when combined with <code>undefined_columns: \"keep\"</code>, allowing dynamically inferred columns to inherit rules automatically.</p> <ul> <li>Maps rules by dataset <code>dtype</code> (<code>string</code>, <code>int</code>, <code>float</code>, <code>date</code>, <code>datetime</code>, <code>boolean</code>).</li> <li>You can declare default <code>on_null</code> tactics and generic <code>constraints</code>.</li> </ul> <p>DO (Global Fallbacks): <pre><code>conf:\n  undefined_columns: \"keep\"\n  defaults:\n    string:\n      on_null:\n        tactic: \"fill_value\"\n        value: \"UNKNOWN\"\n    int:\n      on_null:\n        tactic: \"fill_median\"\n</code></pre> In the example above, any string column lacking an explicit definition will automatically fall back to \"UNKNOWN\" when Null, and inferred integers gracefully median-fill.</p>"},{"location":"02_columns_and_types/","title":"2. Column Typings (<code>columns</code>)","text":"<p>The <code>columns</code> dictionary is the core schema block. Every defined column mandates a <code>dtype</code>. The core pipeline relies fundamentally on Polars native schemas.</p>"},{"location":"02_columns_and_types/#supported-dtypes","title":"Supported <code>dtype</code>s","text":"<ul> <li><code>string</code>: standard text mapping.</li> <li><code>int</code>/<code>float</code>: strict numeric parsing constraints.</li> <li><code>boolean</code>: truthy values.</li> <li><code>date</code> / <code>datetime</code>: Dedicated chronological structs.</li> </ul> <p>DO: <pre><code>columns:\n  user_id:\n    dtype: int\n  is_active:\n    dtype: boolean\n</code></pre></p> <p>DON'T: <pre><code>columns:\n  price:\n    dtype: string # DON'T cast numbers to strings. You cannot run statistical pipelines on string prices!\n</code></pre></p>"},{"location":"02_columns_and_types/#string-preprocessing-trim","title":"String Preprocessing (<code>trim</code>)","text":"<p>Real-world data often suffers from unpredictable leading or trailing whitespace (<code>\"  user_name  \"</code>) and rogue tab characters.</p> <p>By enabling the <code>trim: true</code> directive, <code>detl</code> will apply an optimized, native <code>.strip_chars()</code> operation to every string value inside the column immediately during cast operation. This ensures that downstream string constraints (like <code>max_length</code> or <code>allowed_values</code>) evaluate clean, uniform strings.</p> <pre><code>columns:\n  username:\n    dtype: string\n    trim: true # Safe! \"  Vandal  \" becomes \"Vandal\" before validation!\n</code></pre>"},{"location":"02_columns_and_types/#implicit-type-casting-coercion","title":"Implicit Type Casting (Coercion)","text":"<p><code>detl</code> leverages Polars' native type casting engine (<code>strict=False</code> by default for primitives). This means the engine will attempt to coerce incoming data into your defined <code>dtype</code> automatically if a logical path exists, rather than crashing immediately.</p> <p>Common Implicit Casts: - <code>integer/float</code> \u2794 <code>string</code>: Numbers are cleanly stringified (e.g., <code>12.5</code> becomes <code>\"12.5\"</code>). - <code>integer/float</code> \u2794 <code>boolean</code>: <code>0</code> or <code>0.0</code> translates to <code>false</code>, while any non-zero number translates to <code>true</code>. - <code>string</code> \u2794 <code>numeric</code>: The engine will attempt to parse valid number strings. Unparseable strings become <code>null</code> and are subsequently handled by your <code>on_null</code> tactic.</p> <p>Note: If you are relying on strict zero-to-false mappings (e.g., mapping <code>Catering_Cost: 100</code> to <code>boolean: true</code>), ensure this behavior is intentional.</p>"},{"location":"02_columns_and_types/#date-formatting-format","title":"Date Formatting (<code>format</code>)","text":"<p>If <code>dtype</code> is <code>date</code> or <code>datetime</code>, a specialized parsing block <code>format</code> can be attached to guide formatting and handle unparseable strings.</p> <p>DO (Safe Fallback Parsing): <pre><code>columns:\n  created_at:\n    dtype: datetime\n    format:\n      input: \"%Y-%m-%d %H:%M:%S\"\n      output: \"%Y-%m-%d\" # Automatically truncates the time!\n      on_parse_error:\n        tactic: \"drop_row\" # Silently removes bad timestamp strings like \"N/A\"\n</code></pre></p> <p>DON'T (Hard Failing): <pre><code>columns:\n  created_at:\n    dtype: datetime\n    format:\n      input: \"%Y-%m-%d\"\n      on_parse_error:\n        tactic: \"fail\" # CAUTION! If someone types \"2023-13-40\", the engine immediately crashes.\n</code></pre></p>"},{"location":"02_columns_and_types/#date-parsing-errors-on_parse_error","title":"Date Parsing Errors (<code>on_parse_error</code>)","text":"<p>When dealing with temporal strings, the source data might contain unparseable garbage like <code>\"N/A\"</code>, <code>\"missing\"</code>, or illogical dates like <code>\"2024-13-45\"</code>.</p> <p>The <code>format.on_parse_error</code> configuration allows you to define exactly how <code>detl</code> evaluates these dirty values via two explicit tactics: - <code>drop_row</code>: (Default) The engine will safely convert the unparseable string into a <code>null</code>. If a fallback policy (<code>on_null</code>) is defined for this column, the <code>null</code> will subsequently be resolved via the specified fallback. Otherwise, the row maintains a null value or fails based on standard strictness checks. - <code>fail</code>: The engine acts defensively and aggressively. If any string violates the strictly defined <code>in_format</code>, <code>detl</code> crashes execution instantly. This is extremely important if dropping structural data implies downstream catastrophic failure.</p>"},{"location":"02_columns_and_types/#robust-date-handling-and-fallbacks","title":"Robust Date Handling and Fallbacks","text":"<p>When specifying fallback values (<code>fill_value</code>) for <code>date</code> or <code>datetime</code> types in your configuration (either globally via <code>defaults</code> or specifically per column), <code>detl</code> strictly enforces type and formatting mapping.</p> <p>You must provide the fallback value as a string that strictly matches the <code>date_format.input</code> defined for the column. The execution engine dynamically parses your string fallback into a native temporal struct before imputation, avoiding silent type crashes.</p> <p>DO (Correct Typed Fallback): <pre><code>columns:\n  created_at:\n    dtype: date\n    format:\n      input: \"%Y-%m-%d\"\n    on_null:\n      tactic: \"fill_value\"\n      value: \"2024-01-01\" # Valid!\n</code></pre></p> <p>DON'T (Invalid Type Fallback): <pre><code>columns:\n  created_at:\n    dtype: date\n    format:\n      input: \"%Y-%m-%d\"\n    on_null:\n      tactic: \"fill_value\"\n      value: 20240101 # FAILS: detl will natively block numeric fallbacks on date columns!\n</code></pre></p>"},{"location":"02_columns_and_types/#output-formatting-and-aliasing-rename","title":"Output Formatting and Aliasing (<code>rename</code>)","text":"<p>Because <code>detl</code> is fundamentally a structural ETL orchestrator, native Polars datatypes are maintained strictly in memory through the entire transformational mapping pipeline.</p> <p>However, sometimes you need the exported shape of the data to differ from internal evaluation formats. <code>detl</code> applies structural output mutations as the absolute final step before pushing data to a Connector.</p>"},{"location":"02_columns_and_types/#date-output-formatting","title":"Date Output Formatting","text":"<p>If a column is typed as <code>date</code> or <code>datetime</code>, and a <code>format.output</code> is defined alongside the parser format, <code>detl</code> will stringify the native Temporal object specifically to the assigned schema immediately before writing to Sinks like CSVs or external APIs.</p> <pre><code>columns:\n  birth_date:\n    dtype: date\n    format: \n      input: \"%Y-%m-%d\" # Read source as ISO\n      output: \"%d.%m.%Y\" # Push output as European format (e.g. 10.10.2025)\n</code></pre>"},{"location":"02_columns_and_types/#column-renaming","title":"Column Renaming","text":"<p>To map an evaluated column to a different alias in the target system (e.g., standardizing Legacy SQL naming conventions to generic API payloads), use the <code>rename</code> directive. </p> <pre><code>columns:\n  Legacy_Database_Timestamp_Field:\n    rename: \"created_at\" # Output connector will write the datastream using this property name\n    dtype: datetime\n</code></pre>"},{"location":"03_null_tactics/","title":"3. Handling Missing Data (<code>on_null</code>)","text":"<p>Defines how native <code>null</code> arrays are imputed.</p> <p>Available <code>tactic</code> options: 1. <code>drop_row</code>: Drops the entire row containing a missing value. 2. <code>fail</code>: Hard-aborts pipeline (Be very cautious using this on user-provided streams). 3. <code>fill_value</code>: Hardcodes a static given parameter <code>value</code>. (e.g., <code>value: \"Unknown\"</code>). 4. <code>fill_mean</code>: Replaces nulls with the average (Requires numerical <code>int</code> / <code>float</code> columns). 5. <code>fill_median</code>: Replaces nulls with the median (Better for skewed numerical distributions like Income). 6. <code>fill_min</code>: Replaces nulls with the minimum value of the column (Requires numerical or date). 7. <code>fill_max</code>: Replaces nulls with the maximum value of the column (Requires numerical or date). 8. <code>fill_most_frequent</code>: Replaces nulls with the string or int mode of the column. 9. <code>ffill</code>: Forward-fills linearly. 10. <code>bfill</code>: Backward-fills linearly.</p> <p>DO (Intelligent Median Imputation): <pre><code>columns:\n  salary:\n    dtype: float\n    on_null:\n      tactic: \"fill_median\" # Safely estimates a salary for missing users without skewing the dataset with extremes.\n</code></pre></p> <p>DO (Fallbacks for Strings): <pre><code>columns:\n  country:\n    dtype: string\n    on_null:\n      tactic: \"fill_value\"\n      value: \"Unknown\" # Defaults empty strings to strictly 'Unknown'\n</code></pre></p> <p>DON'T (Logical Fallacies): <pre><code>columns:\n  username:\n    dtype: string\n    on_null:\n      tactic: \"fill_mean\" # DON'T DO THIS. You cannot calculate the 'average' of a string. Pydantic will block this manifest immediately.\n</code></pre></p> <p>DON'T (Missing Required Params): <pre><code>columns:\n  role:\n    dtype: string\n    on_null:\n      tactic: \"fill_value\" \n      # DON'T DO THIS! You forgot to supply the `value: \"guest\"` property. Pydantic will block this manifest.\n</code></pre></p>"},{"location":"04_constraints/","title":"4. Constraint Enforcement (<code>constraints</code>)","text":"<p>Constraints define strict boundaries that evaluate the validity of data and trigger policies (<code>violate_action</code>) if those thresholds are crossed.</p> <p>Every constraint mandates a <code>violate_action</code>. Available <code>violate_action.tactic</code>: - <code>drop_row</code>: Obliterates the offending row. - <code>fail</code>: Aborts engine. - <code>fill_value</code>: Overrides the offending cell with <code>violate_action.value: &lt;str|int|float&gt;</code>. - <code>fill_min</code>: Replaces offending records (like underages) with the absolute lowest non-offending value. - <code>fill_max</code>: Replaces offending records with the highest value.</p>"},{"location":"04_constraints/#numerical-boundaries-min_policy-and-max_policy","title":"Numerical Boundaries: <code>min_policy</code> and <code>max_policy</code>","text":"<p>Requires a <code>threshold: &lt;float|int&gt;</code>. Strict Pydantic validators will abort if this is mapped to a <code>string</code> column.</p> <p>DO (Fallback Violators intelligently): <pre><code>columns:\n  age:\n    dtype: int\n    constraints:\n      min_policy:\n        threshold: 18\n        violate_action:\n          tactic: \"fill_value\"\n          value: 18 # Anyone under 18 is artificially capped to 18 minimum.\n</code></pre></p> <p>DON'T (String Fallacies): <pre><code>columns:\n  name:\n    dtype: string\n    constraints:\n      max_policy:\n        threshold: 15 # You cannot limit a text string using mathematical `max_policy`. Use `max_length`!\n</code></pre></p>"},{"location":"04_constraints/#string-limits-min_length-and-max_length","title":"String Limits: <code>min_length</code> and <code>max_length</code>","text":"<p>Requires a <code>length: &lt;int&gt;</code>. Drop titles that are too short or truncate/drop bloated texts.</p> <p>DO: <pre><code>columns:\n  review_text:\n    dtype: string\n    constraints:\n      max_length:\n        length: 500\n        violate_action:\n          tactic: \"drop_row\" # Drops overly bloated spam reviews entirely.\n</code></pre></p>"},{"location":"04_constraints/#standardized-formats-regex","title":"Standardized Formats: <code>regex</code>","text":"<p>Matches cell values against custom REGEX patterns. Great for strictly formatting IDs, SSNs, or Phone Numbers. Requires <code>pattern: &lt;string&gt;</code>.</p> <p>DO (Regex Strict Checks): <pre><code>columns:\n  postal_code:\n    dtype: string\n    constraints:\n      regex:\n        pattern: '^\\d{5}$'\n        violate_action:\n          tactic: \"fail\" # Immediately breaks if postal boundaries are broken, since this defines core metrics in downstream DWs.\n</code></pre></p>"},{"location":"04_constraints/#categorical-boundaries-allowed_values","title":"Categorical Boundaries: <code>allowed_values</code>","text":"<p>Ensures all values conform strictly to a categorical Enum. - Config 1: <code>values: [\"A\", \"B\", \"C\"]</code> directly inside YAML. - Config 2: <code>source: \"dictionary.csv\"</code> checks values against a 1D CSV list.</p> <p>DO (Enforce Enums &amp; Dicts safely): <pre><code>columns:\n  role:\n    dtype: string\n    constraints:\n      allowed_values:\n        source: \"valid_roles.csv\"\n        separator: \";\" # Optional. Defaults to \",\". ONLY valid for .txt and .csv files. Use it if your dictionary uses a custom delimiter.\n        violate_action:\n          tactic: \"fill_value\"\n          value: \"guest\" # Catches dirty inputs (\"AdmIn\", \"guest1\") and safely maps them to a generic \"guest\".\n</code></pre></p> <p>DON'T (Separators for Numpy arrays): <pre><code>columns:\n  status:\n    dtype: string\n    constraints:\n      allowed_values:\n        source: \"allowed_statuses.npy\"\n        separator: \",\" # DON'T DO THIS! Numpy binary files (.npy) don't use string delimiters. Pydantic will block this manifest immediately.\n        violate_action:\n          tactic: \"drop_row\"\n</code></pre></p>"},{"location":"04_constraints/#uniqueness-unique","title":"Uniqueness: <code>unique</code>","text":"<p>Identifies columns that strictly must carry unique identities (e.g. Primary Keys). - Tactics: <code>drop_extras</code> (keep first row) or <code>fail</code>.</p> <p>DO (Drop duplicates smoothly): <pre><code>columns:\n  user_id:\n    dtype: string\n    constraints:\n      unique:\n        tactic: \"drop_extras\" # If a duplicate ID shows up mid-stream, gently remove it without breaking the pipeline.\n</code></pre></p>"},{"location":"04_constraints/#custom-sql-evaluations-custom_expr","title":"Custom SQL Evaluations: <code>custom_expr</code>","text":"<p>Provides raw SQL boolean expression capacity. Best used for cross-column logic if isolated constraints fall short.</p> <p>DO (Cross-Column Boolean Enforcement): <pre><code>columns:\n  tax_logic:\n    dtype: boolean\n    constraints:\n      custom_expr:\n        expr: \"tax_bracket &gt; 0 AND income &gt; 15000\"\n        violate_action:\n          tactic: drop_row # Erases the record if they declare taxes while under poverty limit\n</code></pre></p> <p>DON'T (SQL Overuse): <pre><code>columns:\n  score:\n    dtype: int\n    constraints:\n      custom_expr:\n        expr: \"score &gt; 10 AND score &lt; 100\" \n        # DON'T DO THIS! Use native `min_policy` and `max_policy` instead!\n        # Why? The engine natively evaluates min/max at the C++ level.\n        # `custom_expr` forcibly engages the heavy SQLContext overhead.\n</code></pre></p>"},{"location":"05_pipeline/","title":"5. Pipeline Transformations (<code>pipeline</code>)","text":"<p>The Pipeline operates sequentially after all schemas, constraints, duplicate logic, and null policies have fired on the Polars <code>LazyFrame</code>. By the time these sequences execute, the data is guaranteed to be clean.</p> <p>Note: All pipeline mutations are resolved heavily natively in a lazy SQLContext.</p>"},{"location":"05_pipeline/#filter","title":"<code>filter</code>","text":"<p>Accepts a raw SQL WHERE-clause string.</p> <p>DO: <pre><code>pipeline:\n  - filter: \"age &gt;= 18 AND status = 'active'\" # Filters out rows extremely fast on the LazyFrame.\n</code></pre></p>"},{"location":"05_pipeline/#mutate","title":"<code>mutate</code>","text":"<p>Accepts a dictionary mapping <code>new_column: SQL AS expression</code>. It natively modifies or appends new columns.</p> <p>DO (Business Aggergations): <pre><code>pipeline:\n  - mutate:\n      total_revenue: \"price * volume\"\n      is_vip: \"total_revenue &gt; 10000\"\n</code></pre></p> <p>DON'T (Data Cleaning): <pre><code>pipeline:\n  - mutate:\n      age: \"COALESCE(age, 32)\" \n      # DON'T MIX CLEANING WITH PIPELINES!\n      # Use `columns.age.on_null` schema block with tactic: \"fill_median\" instead. \n</code></pre></p>"},{"location":"05_pipeline/#rename","title":"<code>rename</code>","text":"<p>Accepts a dictionary mapping <code>old_name: new_name</code>.</p> <p>DO: <pre><code>pipeline:\n  - rename:\n      old_legacy_uuid: \"id\"\n</code></pre></p>"},{"location":"05_pipeline/#sort","title":"<code>sort</code>","text":"<p>Performs <code>LazyFrame.sort()</code>. Requires a <code>by: \"column\"</code> parameter, and an optional <code>order: \"asc\" | \"desc\"</code> parameter (defaults to <code>asc</code>).</p> <p>DO: <pre><code>pipeline:\n  - sort:\n      by: \"total_revenue\"\n      order: \"desc\"\n</code></pre></p>"},{"location":"06_connectors/","title":"Connectors &amp; Extract, Transform, Load (ETL) Routing","text":"<p>The beauty of <code>detl</code> lies in its completely decoupled Extract and Load components. The core processing logic operates on an abstract Polars LazyFrame, meaning you can plug practically ANY combination of sources and sinks to hydrate and export your data safely.</p> <p>No monstrous if-else branching or tightly-coupled framework code. Everything implements a simple <code>Source</code> or <code>Sink</code> interface.</p>"},{"location":"06_connectors/#natively-supported-connectors","title":"Natively Supported Connectors","text":""},{"location":"06_connectors/#file-systems","title":"File Systems","text":"<ul> <li>CSV: <code>CsvSource</code>, <code>CsvSink</code></li> <li>Parquet: <code>ParquetSource</code>, <code>ParquetSink</code> </li> <li>Excel: <code>ExcelSource</code>, <code>ExcelSink</code> </li> </ul>"},{"location":"06_connectors/#databases-powered-by-adbc-connectorx","title":"Databases (Powered by ADBC &amp; ConnectorX)","text":"<ul> <li>Postgres: <code>PostgresSource</code>, <code>PostgresSink</code></li> <li>MySQL: <code>MySQLSource</code>, <code>MySQLSink</code></li> <li>SQLite: <code>SQLiteSource</code>, <code>SQLiteSink</code></li> </ul> <p>(All database interactions are fully zero-copy, streaming data directly into Apache Arrow memory formats without Python iteration overheads).</p>"},{"location":"06_connectors/#3-s3-minio-api","title":"3. S3 / MinIO API","text":"<p>Powered natively by <code>boto3</code> delegating raw bytes directly into Polars via <code>io.BytesIO</code>. Requires <code>boto3</code>.</p> <pre><code>detl --config contract.yml \\\n     --source-type s3 --source-uri \"s3://my-bucket/data.parquet\" \\\n     --sink-type s3 --sink-uri \"s3://processed-bucket/output.csv\" --s3-endpoint-url \"http://localhost:9000\"\n</code></pre>"},{"location":"06_connectors/#performance-limitations","title":"Performance &amp; Limitations","text":""},{"location":"06_connectors/#1-batch-control","title":"1. Batch Control","text":"<p>For large DB loads, you can implement chunking: <pre><code>sink = PostgresSink(..., batch_size=50000)\n</code></pre> Or via CLI: <code>--source-batch-size 100000 --sink-batch-size 50000</code></p>"},{"location":"06_connectors/#2-mysql-sqlalchemy-constraints","title":"2. MySQL <code>sqlalchemy</code> constraints","text":"<p>While PostgeSQL and SQLite can natively stream writes through bleeding-edge <code>adbc</code> bindings natively linked by Polars, MySQL fallback write operations (<code>MySQLSink</code>) rely on <code>sqlalchemy</code>. Therefore, writing to a MySQL sink natively currently demands the installation of <code>pandas</code>.</p>"},{"location":"06_connectors/#python-api-usage","title":"Python API Usage","text":"<p>When integrating <code>detl</code> natively into Airflow or Prefect flows, you can utilize the <code>Processor</code> and <code>Config</code> classes directly:</p> <pre><code>from detl import Processor, Config\nfrom detl.connectors import PostgresSource, ParquetSink\n\n# 1. Load your Declarative Specification\nconfig = Config(\"contract.yml\")\n\n# 2. Define Extractor &amp; Loader bounds\nsource = PostgresSource(\n    connection_uri=\"postgresql://user:pass@localhost:5432/db\", \n    query=\"SELECT * FROM raw_events\"\n)\nsink = ParquetSink(\"s3://data-lake/clean_events.parquet\")\n\n# 3. Fire the pipeline!\nproc = Processor(config)\nproc.execute(source, sink)\n</code></pre>"},{"location":"06_connectors/#cli-usage","title":"CLI Usage","text":"<p>The <code>detl</code> CLI makes it trivial to route inputs and outputs directly from the shell without deploying Airflow configurations.</p>"},{"location":"06_connectors/#simple-file-to-file-legacy","title":"Simple File to File (Legacy)","text":"<pre><code>detl -f auth_manifest.yml -i ./raw_users.csv -o ./clean_users.parquet\n</code></pre>"},{"location":"06_connectors/#advanced-matrix-deployments","title":"Advanced Matrix Deployments","text":"<p>Extract from SQLite -&gt; Transform via Configuration Contract -&gt; Load to Postgres. <pre><code>detl --config contract.yml \\\n     --source-type sqlite --source-uri \"sqlite:///events.db\" --source-query \"SELECT * FROM web_events\" \\\n     --sink-type postgres --sink-uri \"postgresql://admin:password@prod-db.internal/analytics\" --sink-table \"web_events_clean\"\n</code></pre></p>"},{"location":"06_connectors/#creating-custom-connectors","title":"Creating Custom Connectors","text":"<p>If you have an aggressive internal data-lake or proprietary API (such as HubSpot or Salesforce), you can build a native connector rapidly by inheriting from the <code>detl.connectors.Source</code> and <code>detl.connectors.Sink</code> Abstract Base Classes:</p> <pre><code>import polars as pl\nfrom detl.connectors import Source\n\nclass SalesforceSource(Source):\n    ...\n</code></pre>"},{"location":"07_connector_api_reference/","title":"7. Connector API &amp; CLI Reference","text":"<p><code>detl</code> uses a fully decoupled architecture routing data through the <code>detl.connectors</code> abstraction boundaries. What makes <code>detl</code> unique is Matrix Routing: you can pipe data from any Source to any Sink directly without ever saving intermediate files locally!</p>"},{"location":"07_connector_api_reference/#cli-crash-course-matrix-routing","title":"CLI Crash Course (Matrix Routing)","text":"<p>The true power of <code>detl</code> is executed via the CLI. All operations require a path to the Contract (<code>-f config.yaml</code>).</p>"},{"location":"07_connector_api_reference/#1-local-files-the-defaults","title":"1. Local Files (The Defaults)","text":"<p>Use <code>-i</code> and <code>-o</code> as quick shortcuts for File System connectors (CSV, Parquet, Excel).</p> <pre><code># Convert a CSV to Parquet while applying validation schema\nuv run detl -f conf.yaml -i raw_data.csv -o clean_data.parquet\n</code></pre>"},{"location":"07_connector_api_reference/#2-database-extraction","title":"2. Database Extraction","text":"<p>Use <code>--source-type</code> and <code>--source-uri</code> to execute SQL queries natively directly into the pipeline!</p> <pre><code># Extract messy records from Postgres, clean them, and save to CSV\nuv run detl -f conf.yaml \\\n  --source-type postgres \\\n  --source-uri \"postgresql://user:password@localhost:5432/detldb\" \\\n  --source-query \"SELECT * FROM users\" \\\n  -o clean_db.csv\n</code></pre>"},{"location":"07_connector_api_reference/#3-database-to-database-or-db-to-s3","title":"3. Database to Database (Or DB to S3)","text":"<p>You don't need <code>-o</code>! Just specify <code>--sink-type</code> to push data across domains.</p> <pre><code># Read from Postgres -&gt; Validate -&gt; Write cleanly into MySQL\nuv run detl -f conf.yaml \\\n  --source-type postgres \\\n  --source-uri \"postgresql://user:pass@localhost:5432/db1\" \\\n  --source-query \"SELECT * FROM public.orders\" \\\n  --sink-type mysql \\\n  --sink-uri \"mysql+pymysql://user:pass@localhost:3306/db2\" \\\n  --sink-table \"clean_orders\"\n</code></pre>"},{"location":"07_connector_api_reference/#cloud-connectors-s3-minio","title":"Cloud Connectors (S3 / MinIO)","text":"<p>Transfers data in-memory directly utilizing strictly <code>boto3</code>. No local disk space is ever required during pipeline routing.</p>"},{"location":"07_connector_api_reference/#cli-arguments","title":"CLI Arguments","text":"<ul> <li><code>--source-type s3</code> / <code>--sink-type s3</code></li> <li><code>--source-uri \"s3://...\"</code> / <code>--sink-uri \"s3://...\"</code></li> <li><code>--s3-endpoint-url \"http://localhost:9000\"</code> (Optional modifier for custom domains like MinIO)</li> </ul> <pre><code>uv run detl -f conf.yaml -i local.csv --sink-type s3 --sink-uri \"s3://my-bucket/datalake.parquet\" --s3-endpoint-url \"http://localhost:9000\"\n</code></pre>"},{"location":"07_connector_api_reference/#python-api","title":"Python API","text":"<pre><code>from detl.connectors import S3Source, S3Sink\n\nsource = S3Source(\n    s3_uri=\"s3://my-bucket/data.parquet\",\n    format=\"parquet\", # 'csv' or 'parquet'\n    aws_access_key_id=\"admin\", # Or relies on ENV vars natively\n    aws_secret_access_key=\"password\",\n    endpoint_url=\"http://localhost:9000\" # For MinIO\n)\n\nsink = S3Sink(\n    s3_uri=\"s3://my-bucket/cleaned.parquet\",\n    format=\"parquet\"\n)\n</code></pre>"},{"location":"07_connector_api_reference/#database-connectors-postgres-mysql-sqlite","title":"Database Connectors (Postgres / MySQL / SQLite)","text":"<p>Native database protocols mapping zero-copy direct queries via <code>connectorx</code> reading protocols and <code>adbc</code> / <code>sqlalchemy</code> optimized writing blocks.</p>"},{"location":"07_connector_api_reference/#cli-arguments_1","title":"CLI Arguments","text":"<ul> <li><code>--source-type postgres</code> | <code>mysql</code> | <code>sqlite</code></li> <li><code>--source-uri \"protocol://credentials...\"</code></li> <li><code>--source-query \"SELECT * FROM...\"</code></li> <li><code>--source-batch-size 1000</code> (Optional)</li> <li><code>--sink-type postgres</code> | <code>mysql</code> | <code>sqlite</code></li> <li><code>--sink-uri \"protocol://credentials...\"</code></li> <li><code>--sink-table \"target_table_name\"</code></li> <li><code>--sink-if-exists replace</code> (Choices: <code>replace</code> (default), <code>append</code>, <code>fail</code>. Dictates idempotency)</li> <li><code>--sink-batch-size 5000</code> (Optional. Caps streaming memory consumption)</li> </ul>"},{"location":"07_connector_api_reference/#fault-tolerance-memory-idempotency","title":"Fault Tolerance, Memory &amp; Idempotency","text":"<p>Database extractors are strictly typed to act predictably when pipelining large data volumes: 1. Idempotency Defaults (<code>--sink-if-exists replace</code>): Running <code>detl</code> twice against the same data pipeline guarantees identical outcomes natively by safely tearing down the target <code>sink-table</code> and rewriting schemas to match the manifest. Override passing <code>append</code> to map new valid entries sequentially. 2. Memory Overflows (<code>--...-batch-size 50000</code>): Avoid OOM (Out Of Memory) node crashes when extracting millions of rows via <code>PostgresSource</code> by strictly declaring the maximum chunk limits. <code>detl</code> streams data in constrained iterations. 3. Hard Failures: If the <code>source</code> table is missing, the API crashes immediately with <code>[error]Source/Sink Connection Error:[/error]</code>, preventing \"phantom runs\". Null tables return error outputs matching strict DB paradigms.</p>"},{"location":"07_connector_api_reference/#python-api_1","title":"Python API","text":"<pre><code>from detl.connectors import PostgresSource, MySQLSink\n\nsource = PostgresSource(\n    connection_uri=\"postgresql://user:pass@host:5432/db\",\n    query=\"SELECT id, name FROM users\",\n    batch_size=50000 # Optional chunking size constraint\n)\n\nsink = MySQLSink(\n    connection_uri=\"mysql://user:pass@host:3306/db\",\n    table_name=\"cleaned_users\",\n    if_table_exists=\"replace\", # 'append' or 'fail'\n    batch_size=100000\n)\n</code></pre> <p>Dependency Requirements: *   Postgres Reads: <code>connectorx</code>, <code>adbc-driver-postgresql</code> *   MySQL Reads: <code>connectorx</code> *   MySQL Writes: <code>sqlalchemy</code>, <code>pymysql</code>, <code>pandas</code> (Pandas utilized purely as an abstraction fallback) *   SQLite: <code>adbc-driver-sqlite</code></p>"},{"location":"07_connector_api_reference/#file-system-connectors-csv-parquet-excel","title":"File System Connectors (CSV / Parquet / Excel)","text":"<p>Polars-native highly tuned binary abstractions pointing directly at physical disk targets. Automatically evaluates formats lazily avoiding out-of-memory evaluation errors.</p>"},{"location":"07_connector_api_reference/#python-api_2","title":"Python API","text":"<pre><code>from detl.connectors import CsvSource, ParquetSink\n\nsource = CsvSource(\n    path=\"./local_data/input.csv\",\n    separator=\",\" # Auto-defaults to comma\n)\n\nsink = ParquetSink(\n    path=\"./local_data/output_cleaned.parquet\",\n    streaming=True # Enforces streaming API evaluation dynamically against LazyFrames\n)\n</code></pre>"}]}